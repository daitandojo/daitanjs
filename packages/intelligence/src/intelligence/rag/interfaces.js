// intelligence/src/intelligence/rag/interfaces.js
/**
 * @file Defines shared type definitions and conceptual interfaces for the RAG system.
 * @module @daitanjs/intelligence/rag/interfaces
 *
 * @description
 * This file centralizes the JSDoc type definitions and conceptual interfaces used across
 * the RAG (Retrieval Augmented Generation) modules. This promotes consistency and provides
 * a single source of truth for the data structures involved in the RAG pipeline.
 */

/**
 * @typedef {import('./vectorStoreAdapterInterface.js').IVectorStoreAdapter} IVectorStoreAdapter
 * @typedef {import('../core/llmOrchestrator.js').LLMCallbacks} LLMCallbacks
 * @typedef {import('@langchain/core/embeddings').Embeddings} LangChainEmbeddings
 * @typedef {import('../core/llmOrchestrator.js').LLMUsageInfo} LLMUsageInfo
 */

/**
 * @typedef {Object} RetrievalDocument
 * @property {string} pageContent - The main text content of the document.
 * @property {Object} metadata - Metadata associated with the document.
 * @property {number} [score] - Relevance score from the initial retrieval (e.g., vector similarity).
 * @property {number} [relevanceScore] - Refined relevance score from an LLM re-ranker.
 * @property {string} [retrieverType] - Identifier for the type of retriever that found this doc.
 */

/**
 * @typedef {Object} RetrievalResult
 * @property {string} text - The synthesized answer from the LLM.
 * @property {RetrievalDocument[]} retrievedDocs - The final documents used for synthesis after retrieval and re-ranking.
 * @property {string} [originalQuery] - The user's original query.
 * @property {string[]} [generatedSubQueries] - Sub-queries generated by the agent for iterative research.
 * @property {LLMUsageInfo | null} [hydeUsage] - LLM usage for HyDE step.
 * @property {LLMUsageInfo | null} [reRankerUsage] - LLM usage for the re-ranking step.
 * @property {LLMUsageInfo | null} [synthesisUsage] - LLM usage for the final synthesis step.
 */

/**
 * @typedef {Object} LLMConfigForRAG
 * @property {string} [target] - LLM target (e.g., 'FAST_TASKER' or 'provider|model').
 * @property {number} [temperature]
 * @property {number} [maxTokens]
 * // Add other potential overrides from generateIntelligence config.llm
 */

/**
 * @typedef {Object} AskWithRetrievalOptions
 * @property {string} [collectionName] - Target vector store collection.
 * @property {boolean} [persistent=true] - Whether the vector store is persistent (Chroma) or in-memory.
 * @property {LangChainEmbeddings} [embeddings] - Custom embeddings instance.
 * @property {string} [chromaUrl] - Custom URL for ChromaDB server.
 * @property {string} [sessionId] - The session ID for maintaining chat history.
 * @property {LLMCallbacks} [callbacks] - Callbacks for observing the full lifecycle, including sub-steps.
 *
 * @property {number} [topK=5] - The final number of documents to use for synthesis.
 * @property {Object | Function} [filter] - Metadata filter for vector search.
 *
 * @property {boolean} [useHyDE=false] - Whether to use Hypothetical Document Embeddings to improve retrieval.
 * @property {LLMConfigForRAG} [hydeLlmConfig] - Specific LLM config for the HyDE step.
 *
 * @property {boolean} [useLlmReRanker=false] - Whether to use a fast LLM to re-rank initial search results for relevance.
 * @property {LLMConfigForRAG} [reRankerLlmConfig] - Specific LLM config for the re-ranker step.
 *
 * @property {boolean} [useAnalysisPrompt=false] - Use a prompt tailored for data analysis instead of conversational Q&A.
 * @property {boolean} [allowGeneralKnowledge=false] - Allow the final synthesis LLM to use its own knowledge.
 * @property {LLMConfigForRAG} [synthesisLlmConfig] - Specific LLM config for the final answer synthesis.
 *
 * @property {boolean} [localVerbose] - Call-specific verbosity override.
 * @property {boolean} [trackUsage] - Call-specific usage tracking override.
 */

/**
 * @typedef {Object} RagChatInstance
 * @property {(question: string, options?: AskWithRetrievalOptions) => Promise<RetrievalResult>} ask - Asks a question to the RAG system.
 * @property {() => Promise<import('@langchain/core/messages').BaseMessage[]>} getHistory - Retrieves the current chat history for the instance's session.
 * @property {() => void} resetHistory - Clears the chat history for the instance's session.
 * @property {() => IVectorStoreAdapter} getVectorStoreAdapter - Returns the underlying vector store adapter instance.
 */

/**
 * @typedef {Object} EmbedChunksOptions
 * @property {string} [collectionName]
 * @property {string[]} [ids]
 * @property {number} [batchSize]
 * @property {boolean} [localVerbose]
 */

/**
 * @typedef {Object} LoadAndEmbedOptions
 * @property {string} [collectionName]
 * @property {number} [chunkSize=1000]
 * @property {number} [chunkOverlap=200]
 * @property {boolean} [generateAiMetadata=true] - Use an LLM to generate tags and summary for the document.
 * @property {boolean} [useMultiVector=false] - Generate and embed summaries and hypothetical questions for each chunk to improve retrieval quality.
 * @property {boolean} [localVerbose]
 * @property {EmbedChunksOptions} [embedOptions]
 */

export {};
